{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try use requests\n",
    "def get_soup(url):\n",
    "    page=requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    return soup\n",
    "url=\"https://www.indeed.nl/jobs?q=innovation+manager&l=Nederland\"\n",
    "soup=get_soup(url)\n",
    "#print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_soup(url):\n",
    "#     \"\"\"\n",
    "#     Given the url of a page, this function returns the soup object.\n",
    "    \n",
    "#     Parameters:\n",
    "#         url: the link to get soup object for\n",
    "    \n",
    "#     Returns:\n",
    "#         soup: soup object\n",
    "#     \"\"\"\n",
    "#     driver = webdriver.Chrome()\n",
    "#     driver.get(url)\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     driver.close()\n",
    "    \n",
    "#     return soup\n",
    "# url=\"https://www.indeed.nl/jobs?q=innovation+manager&l=Nederland\"\n",
    "# soup=get_soup(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_job_links(soup):\n",
    "    \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "        urls: a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Loop thru all the posting links\n",
    "    for link in soup.find_all('div', {'class': 'title'}):\n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        partial_url = link.a.get('href')\n",
    "        # This is a partial url, we need to attach the prefix\n",
    "        url = 'https://www.indeed.nl'+partial_url\n",
    "        # Make sure this is not a sponsored posting\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls\n",
    "#grab_job_links(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(query, num_pages, location):\n",
    "    \"\"\"\n",
    "    Get all the job posting URLs resulted from a specific search.\n",
    "    \n",
    "    Parameters:\n",
    "        query: job title to query\n",
    "        num_pages: number of pages needed\n",
    "        location: city to search in\n",
    "    \n",
    "    Returns:\n",
    "        urls: a list of job posting URL's (when num_pages valid)\n",
    "        max_pages: maximum number of pages allowed ((when num_pages invalid))\n",
    "    \"\"\"\n",
    "    # We always need the first page\n",
    "    base_url = 'https://www.indeed.nl/jobs?q={}&l={}'.format(query, location)\n",
    "    soup = get_soup(base_url)\n",
    "    urls = grab_job_links(soup)\n",
    "    \n",
    "    # Get the total number of postings found \n",
    "    posting_count_string = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n",
    "    posting_count_string = posting_count_string[posting_count_string.find('van')+2:].strip()\n",
    "    #print('posting_count_string: {}'.format(posting_count_string))\n",
    "    #print('type is: {}'.format(type(posting_count_string)))\n",
    "    \n",
    "    try:\n",
    "        posting_count = int(posting_count_string)\n",
    "    except ValueError: # deal with special case when parsed string is \"360 jobs\"\n",
    "        posting_count = int(re.search('\\d+', posting_count_string).group(0))\n",
    "       # print('posting_count: {}'.format(posting_count))\n",
    "       # print('\\ntype: {}'.format(type(posting_count)))\n",
    "    finally:\n",
    "        posting_count = 330 # setting to 330 when unable to get the total\n",
    "        pass\n",
    "    \n",
    "    # Limit nunmber of pages to get\n",
    "    max_pages = round(posting_count / 10) - 3\n",
    "    if num_pages > max_pages:\n",
    "        print('returning max_pages!!')\n",
    "        return max_pages\n",
    "    \n",
    "        # Additional work is needed when more than 1 page is requested\n",
    "    if num_pages >= 2:\n",
    "        # Start loop from page 2 since page 1 has been dealt with above\n",
    "        for i in range(2, num_pages+1):\n",
    "            num = (i-1) * 10\n",
    "            base_url = 'https://www.indeed.nl/jobs?q={}&l={}&start={}'.format(query, location, num)\n",
    "            try:\n",
    "                soup = get_soup(base_url)\n",
    "                # We always combine the results back to the list\n",
    "                urls += grab_job_links(soup)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Check to ensure the number of urls gotten is correct\n",
    "    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n",
    "\n",
    "    return urls     \n",
    "#get_urls('innovation manager optic', 1, 'Nederland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting(url):\n",
    "    \"\"\"\n",
    "    Get the text portion including both title and job description of the job posting from a given url\n",
    "    \n",
    "    Parameters:\n",
    "        url: The job posting link\n",
    "        \n",
    "    Returns:\n",
    "        title: the job title (if \"data scientist\" is in the title)\n",
    "        posting: the job posting content    \n",
    "    \"\"\"\n",
    "    # Get the url content as BS object\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # The job title is held in the h3 tag\n",
    "    title = soup.find(name='h3').getText().lower()\n",
    "    posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent-description icl-u-xs-mt--md\"}).get_text()\n",
    "\n",
    "    return title, posting.lower()\n",
    "\n",
    " #if 'data scientist' in title:  # We'll proceed to grab the job posting text if the title is correct\n",
    "        # All the text info is contained in the div element with the below class, extract the text.\n",
    "        #posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "        #return title, posting.lower()\n",
    "    #else:\n",
    "        #return False\n",
    "    \n",
    "        # Get rid of numbers and symbols other than given\n",
    "        #text = re.sub(\"[^a-zA-Z'+#&]\", \" \", text)\n",
    "        # Convert to lower case and split to list and then set\n",
    "        #text = text.lower().strip()\n",
    "    \n",
    "        #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(query, num_pages, location='Nederland'):\n",
    "    \"\"\"\n",
    "    Get all the job posting data and save in a json file using below structure:\n",
    "    \n",
    "    {<count>: {'title': ..., 'posting':..., 'url':...}...}\n",
    "    \n",
    "    The json file name has this format: \"\"<query>.json\"\n",
    "    \n",
    "    Parameters:\n",
    "        query: Indeed query keyword such as 'Data Scientist'\n",
    "        num_pages: Number of search results needed\n",
    "        location: location to search for\n",
    "    \n",
    "    Returns:\n",
    "        postings_dict: Python dict including all posting data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the queried title to Indeed format\n",
    "    query = '+'.join(query.lower().split())\n",
    "    \n",
    "    postings_dict = {}\n",
    "    urls = get_urls(query, num_pages, location)\n",
    "    \n",
    "    #  Continue only if the requested number of pages is valid (when invalid, a number is returned instead of list)\n",
    "    if isinstance(urls, list):\n",
    "        num_urls = len(urls)\n",
    "        for i, url in enumerate(urls): #enumerate explain: http://book.pythontips.com/en/latest/enumerate.html\n",
    "            try:\n",
    "                title, posting = get_posting(url)\n",
    "                \n",
    "                #an empty dictionary, check here to know more:https://docs.python.org/2/tutorial/datastructures.html\n",
    "                postings_dict[i] = {} \n",
    "                \n",
    "                postings_dict[i]['title'], postings_dict[i]['posting'], postings_dict[i]['url'] = \\\n",
    "                title, posting, url\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            percent = (i+1) / num_urls\n",
    "            # Print the progress the \"end\" arg keeps the message in the same line \n",
    "            print(\"Progress: {:2.0f}%\".format(100*percent), end='\\r')\n",
    "\n",
    "        # Save the dict as json file\n",
    "        file_name = query.replace('+', '_') + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(postings_dict, f)\n",
    "        \n",
    "        print('All {} postings have been scraped and saved!'.format(num_urls))    \n",
    "        #return postings_dict\n",
    "    else:\n",
    "        print(\"Due to similar results, maximum number of pages is only {}. Please try again!\".format(urls))\n",
    "\n",
    "\n",
    "\n",
    "# If script is run directly, we'll take input from the user\n",
    "# if __name__ == \"__main__\":\n",
    "#     queries = [\"Innovation manager optic\", \"machine learning engineer\", \"data engineer\"]\n",
    "    \n",
    "#     while True: \n",
    "#         query = input(\"Please enter the title to scrape data for: \\n\").lower()\n",
    "#         if query in queries:\n",
    "#             break\n",
    "#         else:\n",
    "#             print(\"Invalid title! Please try again.\")\n",
    "\n",
    "#     while True:\n",
    "#         num_pages = input(\"Please enter the number of pages needed (integer only): \\n\")\n",
    "#         try:\n",
    "#             num_pages = int(num_pages)\n",
    "#             break\n",
    "#         except:\n",
    "#             print(\"Invalid number of pages! Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 16 postings have been scraped and saved!\n"
     ]
    }
   ],
   "source": [
    "get_data('Researcher', 1, location='Nederland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
